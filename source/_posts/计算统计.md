---
title: è®¡ç®—ç»Ÿè®¡å’Œè´å¶æ–¯ç»Ÿè®¡ç¬”è®°
tags: 
  - "statistics"
  - "ç»Ÿè®¡"
categories: Math
keywords: Beyesian Statistics, Computing Statistics
description: æœ¬æ–‡æ˜¯å…³äºè®¡ç®—ç»Ÿè®¡ã€è´å¶æ–¯ç»Ÿè®¡çš„è¯¾ç¨‹ç¬”è®°ã€‚
cover: https://picture-1302512218.cos.ap-guangzhou.myqcloud.com/6-27/Pure-mathematics-formulÃ¦-blackboard.jpg
date: 2019-01-02 14:44:47
katex: true
---

----

# ç›®å½•

``` Markdown
1. ç›®å½•
2. ç›¸å…³å…ˆä¿®å†…å®¹ (å¾…è¡¥å……)
    2.1. æ¦‚ç‡
    2.2. å¤§æ•°å®šå¾‹ Law of Large Numbers
    2.3. ä¸­å¿ƒæé™å®šå¾‹ Central Limit Theorem
    2.4. å…¶ä»– (å¾…æ›´)
    2.5. è¡¥å……
3. å„ç§æ¦‚ç‡åˆ†å¸ƒ (å¾…è¡¥å……)
    3.1. ç¦»æ•£å‹åˆ†å¸ƒ
    3.2. è¿ç»­å‹åˆ†å¸ƒ
4. æŠ½æ ·æ–¹æ³•
    4.1. The Inversion Method
    4.2. The Grid Method
    4.3. The Rejection Method
    4.4. The Sampling/Importance Resampling Method
    4.5. The Stochastic Representation Method
    4.6. The Conditional Sampling Method
5. ç»Ÿè®¡ä¸­çš„ä¼˜åŒ–ç®—æ³• (å¾…æ›´)
    5.1. åŸºç¡€
    5.2. The Newton-Raphson Algorithm
    5.3. The Fisher Scoring Algorithm
    5.4. The EM Algorithm
    5.5. The ECM Algorithm
    5.6. The MM Algorithms
6. ç»å…¸è’™ç‰¹å¡æ´›ç§¯åˆ†ä¸é»æ›¼å’Œä¼°è®¡ (å¾…æ›´)
    6.1. Classical Monte Carlo Integration
    6.2. The Riemannian Sum Estimator
7. è´å¶æ–¯ç»Ÿè®¡ Bayesian Statistics (å¾…æ›´)
8. éšæœºè¿‡ç¨‹ (å¾…æ›´)
    8.1. é©¬å°”å¯å¤«é“¾
    8.2. å¹³ç¨³è¿‡ç¨‹
8. è’™ç‰¹å¡æ´›æ–¹æ³• MCMC Methods (å¾…æ›´)
9. Bootstrap Methods (å¾…è¡¥å……)
    9.1. Parametric Bootstrap
    9.2. Non-Parametric Bootstrap
    9.3. Hypothesis Testing with the Bootstrapj
```

# ç›¸å…³å…ˆä¿®å†…å®¹

## æ¦‚ç‡

éšæœºå˜é‡ï¼šè®¾ $(\Omega, \mathcal{F}, P)$ æ˜¯æ¦‚ç‡ç©ºé—´ï¼Œ$X=X(\omega)$ æ˜¯å®šä¹‰åœ¨æ ·æœ¬ç©ºé—´$\Omega$ä¸Šçš„å•å€¼å®å‡½æ•°ã€‚å¦‚æœå¯¹äºä»»ä¸€å®æ•°xï¼Œæœ‰

$$ \{\omega | X(\omega)\leq x\} \in \mathcal{F}$$

åˆ™ç§° $X=X(\omega)$ ä¸ºéšæœºå˜é‡ï¼Œç®€è®°Xã€‚

## å¤§æ•°å®šå¾‹ Law of Large Numbers

å¤§æ•°å®šå¾‹ä¸»è¦æœ‰ä¸¤ç§è¡¨ç°å½¢å¼ï¼š**å¼±å¤§æ•°å®šå¾‹**å’Œ**å¼ºå¤§æ•°å®šå¾‹**ã€‚å®šå¾‹çš„ä¸¤ç§å½¢å¼éƒ½è‚¯å®šæ— ç–‘åœ°è¡¨æ˜ï¼Œæ ·æœ¬å‡å€¼

$$ \overline{X_n} = \frac{1}{n} (X_1 + \cdots + X_2) \qquad \overline{X_n} \to \mu \text{ as $n \to \infty$} $$

å…¶ä¸­ ${ {X_1}, {X_2}, \dots }$ æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒã€æœŸæœ›å€¼ ${E(X_1)=E(X_2)=\cdots=\mu}$ ä¸”çš†å‹’è´æ ¼å¯ç§¯çš„éšæœºå˜é‡æ„æˆçš„æ— ç©·åºåˆ—ã€‚ ${X_j}$ çš„å‹’è´æ ¼å¯ç§¯æ€§æ„å‘³ç€æœŸæœ›å€¼ ${E(X_j)}$ å­˜åœ¨ä¸”æœ‰é™ã€‚

### å¼±å¤§æ•°å®šå¾‹

å¼±å¤§æ•°å®šå¾‹(Weak law)ä¹Ÿç§°ä¸º**è¾›é’¦å®šç†** (Khinchin's law)ï¼Œé™ˆè¿°ä¸ºï¼šæ ·æœ¬å‡å€¼ä¾æ¦‚ç‡æ”¶æ•›äºæœŸæœ›å€¼ã€‚
![å¼±å¤§æ•°å®šå¾‹](https://wikimedia.org/api/rest_v1/media/math/render/svg/8fd3613e2003696db3957fe5cbd4b813fa24b255)
ä¹Ÿå°±æ˜¯è¯´å¯¹äºä»»æ„æ­£æ•° ${\epsilon}$ï¼Œæœ‰

$${lim_{n \to \infty} P(\lvert \overline{X_n}-\mu \rvert > \epsilon)=0}$$

### å¼ºå¤§æ•°å®šå¾‹

å¼ºå¤§æ•°å®šå¾‹(Strong law)æŒ‡å‡ºï¼Œæ ·æœ¬å‡å€¼ä»¥æ¦‚ç‡1 (almost surely) æ”¶æ•›äºæœŸæœ›å€¼ã€‚
![å¼ºå¤§æ•°å®šå¾‹](https://wikimedia.org/api/rest_v1/media/math/render/svg/942aac6785e95ea851ccb16312b4244b54788763)
å³ ${P(lim_{n \to \infty} \overline{X_n}=\mu)=1}$.

### å…³äºconverges almost surely å’Œ converges in probability

<div class="note info">**æ®†å¿…æ”¶æ•› converges almost surely**

è®¾æ ·æœ¬ç©ºé—´Sä¸ºé—­åŒºé—´[0,1]ï¼Œä¸”è¯¥åŒºé—´ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒä¸ºå‡åŒ€åˆ†å¸ƒã€‚å®šä¹‰éšæœºå˜é‡$X_n(s)=s+s^n$ä»¥åŠ$X(s)=s$ã€‚å¯¹ä»»æ„$s\in [0,1)$ï¼Œå½“$n\to\infty$æ—¶æœ‰$s^n\to 0$ã€‚è€Œå¯¹ä»»æ„néƒ½æœ‰$X_n(1)=2$ï¼Œæ‰€ä»¥$X_n(1)$ä¸æ”¶æ•›äº1=X(1)ã€‚ä½†ç”±äºåœ¨é›†åˆ[0,1)ä¸Šéƒ½æ”¶æ•›ï¼Œä¸”$Pr([0,1))=1$ï¼Œæ‰€ä»¥$X_n$æ®†å¿…æ”¶æ•›äºXã€‚</div>

<div class="note info">**ä¾æ¦‚ç‡æ”¶æ•›ï¼Œä½†éæ®†å¿…æ”¶æ•› converges in probability**

è®¾æ ·æœ¬ç©ºé—´Sä¸ºé—­åŒºé—´[0,1]ï¼Œä¸”è¯¥åŒºé—´ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒä¸ºå‡åŒ€åˆ†å¸ƒã€‚å®šä¹‰éšæœºå˜é‡åºåˆ—Xå¦‚ä¸‹ï¼š

$$X_1(s)=s+I_{[0,1]}(s)\\
X_2(s)=s+I_{[0,1/2]}(s)\\
X_3(s)=s+I_{[1/2,1]}(s)\\
X_4(s)=s+I_{[0,1/3]}(s)\\
X_5(s)=s+I_{[1/3,2/3]}(s)\\
X_6(s)=s+I_{[2/3,1]}(s)\\
...$$

ä»¤$X(s)=s$ï¼Œç”±äºå½“$n\to\infty$æ—¶æœ‰$Pr(|X_n-X|\geq\epsilon)$ç­‰äºsçš„æŸé•¿åº¦è¶‹äº0çš„åŒºé—´çš„æ¦‚ç‡ï¼Œæ•…$X_n$ä¾æ¦‚ç‡æ”¶æ•›äºXï¼Œä½†$X_n$ä¸æ®†å¿…æ”¶æ•›äºXã€‚äº‹å®ä¸Šï¼Œå¯¹ä»»æ„$s\in S$éƒ½æ²¡æœ‰$X_n(s)\to s=X(s)$ï¼Œå› ä¸ºå¯¹ä»»æ„sï¼Œ$X_n(s)$çš„å€¼åªèƒ½äº¤æ›¿å–sæˆ–è€…s+1ã€‚ä¾‹å¦‚ï¼Œè‹¥s=3/8,åˆ™$X_1(s)=X_2(s)=X_5(s)=\cdots=1+\frac{3}{8}\quad X_3(s)=X_4(s)=X_6(s)=\cdots=\frac{3}{8}$ å¹¶ä¸æ”¶æ•›ã€‚</div>

æ‰€ä»¥æ®†å¿…æ”¶æ•›è•´å«ä¾æ¦‚ç‡æ”¶æ•›ï¼Œä¸è¿‡ï¼Œä¾æ¦‚ç‡æ”¶æ•›çš„éšæœºå˜é‡åºåˆ—å¿…å­˜åœ¨æ®†å¿…æ”¶æ•›çš„**å­åˆ—**(è§Resnick 1999 ç¬¬6.3èŠ‚)ã€‚

![ex1](https://raw.githubusercontent.com/georgeokelly/hello-world/master/blog_images/2019-1-2/11.png)
<p style="text-align:center;font-size:20px">converges almost surely</p>
![ex2](https://raw.githubusercontent.com/georgeokelly/hello-world/master/blog_images/2019-1-2/12.png)
<p style="text-align:center;font-size:20px">converges in probability</p>

<div class="note info">**Note.**

å¾…æ•´ç†
</div>

## ä¸­å¿ƒæé™å®šå¾‹ Central Limit Theorem

å¾…æ›´

## å…¶ä»–

<div class="note success no-icon">åˆ‡æ¯”é›ªå¤«ä¸ç­‰å¼ã€åæ–¹å·®ä¸ç›¸å…³ç³»æ•°</div>

å¾…æ›´

## è¡¥å……

<div class="note info">**å››ä¸ªç»Ÿè®¡æ–¹å‘ (field)**

1. Classical Methods: Likelihood / Frequentist
2. Bayesian Methods
3. Fiducial Methods (ä¿¡ä»»æ–¹æ³•)
4. Bootstrap Methods (è‡ªåŠ©æ–¹æ³•)</div>

<div class="note info">**å››ä¸ªç ”ç©¶å†…å®¹**

1. ç‚¹ä¼°è®¡(point estimate): Constrained MLE / Unconstrained MLE / Generalized Moment Estimate / Robust Estimate
2. åŒºé—´ä¼°è®¡(interval estimate): S+d / Bootstrap
3. å‡è®¾æ£€éªŒ(Testing Hypothesis)
4. æ¨¡å‹çš„æ‹Ÿåˆä¸é€‰æ‹©: goodness of fit test / variable selection / AIC / BIC</div>

<div class="note info">**å››ä¸ªæ£€éªŒæ–¹æ³•**

1. Wald Test
2. Likelihood Ratio Test
3. Score Test
4. Exact Test</div>

<div class="note info">**å››ä¸ªç®—æ³•**

1. Newton-type algorithm
2. EM-type algorithm
3. MM-type algorithm
4. Mode-sharing algorithm</div>

<div class="note info">**å››å¤§æ•°æ®ç±»å‹**

1. Continuouså‹
2. Discreteå‹
3. åŒºé—´å‹ å¦‚(0, 1)å†…çš„è¿ç»­å‹
4. å¯¿å‘½å‹ å¦‚(Censoring), Longitudinal</div>

<div class="note info">**å››å¤§æ¨¡å‹**

1. Without Covaviate (Distribution)
2. With Covaviate (Linear, Generialized, Non-linear)
3. Cox Model
4. Random Effect Models</div>

# å„ç§æ¦‚ç‡åˆ†å¸ƒ

## ç¦»æ•£å‹åˆ†å¸ƒ

**å‡åŒ€åˆ†å¸ƒ X \~ U(a, b)**

æè¿°äº†æœ‰é™ä¸ªæ•°å€¼æ‹¥æœ‰ç›¸åŒæ¦‚ç‡çš„æ¦‚ç‡åˆ†å¸ƒã€‚

æ”¯æ’‘é›† $k \in \{ a, a + 1, a + 2, \ldots, b - 1, b\}$

æ¦‚ç‡è´¨é‡å‡½æ•°

$$p( X = k ) = \left\{ \begin{matrix}
\frac{1}{n} & \textrm{ for a < k < b} \\
0 & \textrm{ otherwise} \\
\end{matrix} \right. \qquad n = b - a + 1$$

ç´¯è®¡åˆ†å¸ƒå‡½æ•°

$$F( k ) = \left\{ \begin{matrix}
0 & \textrm{ k < a} \\
\frac{k - a + 1}{n} & \textrm{ $a \leq k < b$} \\
1 & \textrm{ $b \leq k$} \\
\end{matrix} \right. $$

æ•°å­¦æœŸæœ›

$$E( X ) = \frac{a + b}{2}$$

æ–¹å·®

$$\text{var}( X ) = \frac{n^{2} - 1}{12}$$

**ä¼¯åŠªåˆ©/ä¸¤ç‚¹åˆ†å¸ƒ X \~ Bernoulli(p)**

æè¿°äº†è‹¥ä¼¯åŠªåˆ©è¯•éªŒæˆåŠŸï¼Œåˆ™ä¼¯åŠªåˆ©éšæœºå˜é‡å–å€¼ä¸º1ã€‚è‹¥ä¼¯åŠªåˆ©è¯•éªŒå¤±è´¥ï¼Œåˆ™ä¼¯åŠªåˆ©éšæœºå˜é‡å–å€¼ä¸º0çš„æ¦‚ç‡åˆ†å¸ƒã€‚

æ”¯æ’‘é›† $k \in \{ 0,\ 1\}$

æ¦‚ç‡è´¨é‡å‡½æ•°

$$p( X = k ) = p^{k}{(1 - p)}^{1 - k} = \left\{ \begin{matrix}
1 - p & \textrm{ k = 0 }\\
p & \textrm{ k = 1 }\\
\end{matrix} \right. $$

ç´¯è®¡åˆ†å¸ƒå‡½æ•°

$$F( k ) = \left\{ \begin{matrix}
0 & \textrm{ k < 1 }\\
1 - p & \textrm{ $0 \leq k < 1$} \\
1 & \textrm{ $1 \leq k$} \\
\end{matrix} \right. $$

æ•°å­¦æœŸæœ›

$$E( X ) = p$$

æ–¹å·®

$$\text{var}( X ) = p(1 - p)$$

**äºŒé¡¹åˆ†å¸ƒ X \~ B(n, p)**

æè¿°äº†næ¬¡è¯•éªŒä¸­æ­£å¥½å¾—åˆ°kæ¬¡æˆåŠŸçš„æ¦‚ç‡åˆ†å¸ƒã€‚

æ”¯æ’‘é›† $k \in \{ 0,\ 1,\ \ldots,\ n\}$

æ¦‚ç‡è´¨é‡å‡½æ•°

$$p( X = k;n,p ) = {n\choose k} p^k (1 - p)^{n - k}$$

$${n\choose k} = \frac{n!}{k!( n - k )!}$$

ç´¯è®¡åˆ†å¸ƒå‡½æ•°

$$F( k;n,p ) = \sum_{i = 0}^{k}{ {n\choose i} p^i(1 - p)^{n - i}}$$

æ•°å­¦æœŸæœ›

$$E( X ) = np$$

æ–¹å·®

$$\text{var}( X ) = np(1 - p)$$

**å¤šé¡¹å¼åˆ†å¸ƒ X \~ Multinomial (n, p1, p2, ..., pk)**

æ˜¯äºŒé¡¹åˆ†å¸ƒçš„ä¸€èˆ¬åŒ–ã€‚

æ”¯æ’‘é›†
$x_{i} \in \left\{ 0,\ 1,\ \ldots,\ n \right\}\quad\sum_{}^{}x_{i} = n\quad i \in \{ 1,\ 2,\ \ldots,\ k\}$

æ¦‚ç‡è´¨é‡å‡½æ•°

$$p( \vec{X} = {\{ x_1, x_2, \ldots, x_k\}}^T;n,p ) = \left\{ \begin{matrix}
\frac{n!}{x_1 !\ldots x_k !} p^{x_1}\cdots p^{x_k} & \textrm{ $when\sum_{}^{}x_{i} = n$}  \\
0 & \textrm{ otherwise }\\
\end{matrix} \right. $$

ç´¯è®¡åˆ†å¸ƒå‡½æ•°æ¯”è¾ƒå¤æ‚ã€‚

æ•°å­¦æœŸæœ›

$$E( X_{i} ) = np_{i}$$

æ–¹å·®/åæ–¹å·®

$$\text{var}( X_{i} ) = np_{i}(1 - p_{i})$$

$$\text{Cov}( X_{i},\ X_{j} ) = - np_{i}p_{j}\quad i \neq j$$

**æ³Šæ¾åˆ†å¸ƒ X \~ P(Î»)**

åœ¨äºŒé¡¹åˆ†å¸ƒçš„ä¼¯åŠªåˆ©è¯•éªŒä¸­ï¼Œå¦‚æœè¯•éªŒæ¬¡æ•°nå¾ˆå¤§ï¼ŒäºŒé¡¹åˆ†å¸ƒçš„æ¦‚ç‡på¾ˆå°ï¼Œä¸”ä¹˜ç§¯Î»=
npæ¯”è¾ƒé€‚ä¸­ï¼Œåˆ™äº‹ä»¶å‡ºç°çš„æ¬¡æ•°çš„æ¦‚ç‡å¯ä»¥ç”¨æ³Šæ¾åˆ†å¸ƒæ¥é€¼è¿‘ã€‚

æ”¯æ’‘é›† $k \in \{ 0,\ 1,\ \ldots\}$

æ¦‚ç‡è´¨é‡å‡½æ•°

$$p( X = k ) = \frac{\lambda^k}{k!}e^{- \lambda}$$

ç´¯è®¡åˆ†å¸ƒå‡½æ•°

$$F( k;\lambda ) = e^{- \lambda}\sum_{i = 0}^{k}\frac{\lambda^i}{i!} = \frac{\Gamma( k + 1,\lambda )}{k!}$$

$$\Gamma( s,x ) = \int_{x}^{\infty}{t^{s - 1}e^{- t}dt}$$

æ•°å­¦æœŸæœ›

$$E( X ) = \lambda$$

æ–¹å·®

$$\text{var}( X ) = \lambda$$

**å‡ ä½•åˆ†å¸ƒ X \~ G(p)**

æè¿°äº†åœ¨ä¼¯åŠªåˆ©è¯•éªŒä¸­ï¼Œå¾—åˆ°ä¸€æ¬¡æˆåŠŸæ‰€éœ€è¦çš„è¯•éªŒæ¬¡æ•°æˆ–å¤±è´¥çš„æ¬¡æ•°ã€‚å‰ä¸€ç§å½¢å¼ç»å¸¸è¢«ç§°ä½œshifted
geometric distributionï¼Œå¦‚ä¸‹ï¼š

æ”¯æ’‘é›† $k \in \{ 1,\ 2,\ \ldots\}$

æ¦‚ç‡è´¨é‡å‡½æ•°

$$p( X = k;p ) = p(1 - p)^{k - 1}$$

ç´¯è®¡åˆ†å¸ƒå‡½æ•°

$$F( k;p ) = 1 - (1 - p)^k$$

æ•°å­¦æœŸæœ›

$$E( X ) = 1/p$$

æ–¹å·®

$$\text{var}( X ) = (1 - p)/p^2$$

**è¶…å‡ ä½•åˆ†å¸ƒ X \~ H(n, K, N)**

æè¿°äº†ç”±å«æœ‰Kä¸ªæŒ‡å®šç§ç±»ç‰©ä»¶çš„æœ‰é™Nä¸ªç‰©ä»¶ä¸­æŠ½å‡ºnä¸ªç‰©ä»¶ï¼ŒæˆåŠŸæŠ½å‡ºè¯¥æŒ‡å®šç§ç±»çš„ç‰©ä»¶çš„ä¸ªæ•°kï¼ˆä¸å½’è¿˜
ï¼ˆwithout replacementï¼‰ï¼‰ã€‚

æ”¯æ’‘é›† $k \in \{ 1, 2, \ldots\}$

æ¦‚ç‡è´¨é‡å‡½æ•°

$$p(X=k;n,K,N)=\frac{K\choose k }{N\choose n} {N-K\choose n-k}$$

ç´¯è®¡åˆ†å¸ƒå‡½æ•°æ¯”è¾ƒå¤æ‚

æ•°å­¦æœŸæœ›

$$E( X ) = nK/N$$

æ–¹å·®æ¯”è¾ƒå¤æ‚

## è¿ç»­å‹åˆ†å¸ƒ

**å‡åŒ€åˆ†å¸ƒ X \~ U(a, b)**

æ”¯æ’‘é›† $a \leq x \leq b$

æ¦‚ç‡å¯†åº¦å‡½æ•°

$$f( x ) = \left\{ \begin{matrix}
\frac{1}{b - a} & \textrm{$ for\ a \leq x \leq b$} \\
0 & \textrm{ otherwise} \\
\end{matrix} \right. $$

ç´¯è®¡åˆ†å¸ƒå‡½æ•°

$$F( x ) = Pr(X < x) = \left\{ \begin{matrix}
0 & \textrm{ k < a} \\
\frac{x - a}{b - a} & \textrm{$ a \leq k < b$} \\
1 & \textrm{$ b \leq k$} \\
\end{matrix} \right. $$

æ•°å­¦æœŸæœ›

$$E(X) = \frac{a + b}{2}$$

æ–¹å·®

$$D(X) = \frac{(b - a)^2}{12}$$

**æŒ‡æ•°åˆ†å¸ƒ X \~ Exp(Î»)**

æŒ‡æ•°åˆ†å¸ƒå¯ä»¥ç”¨æ¥è¡¨ç¤ºç‹¬ç«‹éšæœºäº‹ä»¶å‘ç”Ÿçš„æ—¶é—´é—´éš”ã€‚

æ”¯æ’‘é›† $x \in \lbrack 0,\  + \infty)$

æ¦‚ç‡å¯†åº¦å‡½æ•°

$$f( x ) = \lambda e^{- \lambda x}\quad \text{for $x \geq 0$}$$

ç´¯è®¡åˆ†å¸ƒå‡½æ•°

$$F( x ) = Pr(X < x) = \left\{ \begin{matrix}
0 & \textrm{ x < 0} \\
1 - e^{- \lambda x} & \textrm{$ 0 \leq x$} \\
\end{matrix} \right. $$

æ•°å­¦æœŸæœ›

$$E(X) = \frac{1}{\lambda}$$

æ–¹å·®

$$D(X) = \frac{1}{\lambda^2}$$

**æ­£æ€åˆ†å¸ƒ X \~ B(Î¼, Ïƒ^2^)**

æ­£æ€åˆ†å¸ƒåœ¨ç»Ÿè®¡å­¦ä¸Šååˆ†é‡è¦ï¼Œç»å¸¸ç”¨åœ¨è‡ªç„¶å’Œç¤¾ä¼šç§‘å­¦æ¥ä»£è¡¨ä¸€ä¸ªä¸æ˜çš„éšæœºå˜é‡ã€‚

æ”¯æ’‘é›† $x \in ( - \infty,  + \infty)$

æ¦‚ç‡å¯†åº¦å‡½æ•°

$$f( x ) = \frac{1}{\sigma\sqrt{2\pi}}e^{\frac{(x - \mu)^2}{- 2\sigma^2}}$$

ç´¯è®¡åˆ†å¸ƒå‡½æ•°

$$F( x ) = Pr(X < x) = \frac{1}{2}\lbrack 1 + \text{erf}( \frac{x - \mu}{\sigma\sqrt{2}} )\rbrack$$

$$\text{erf}( x ) = \frac{1}{\sqrt{\pi}}\int_{- x}^{x}{e^{- t^{2}}dt}$$

æ•°å­¦æœŸæœ›

$$E(X) = \mu$$

æ–¹å·®

$$D(X) = \sigma^{2}$$

**ä¼½é©¬åˆ†å¸ƒ X \~ Î“(Î±, Î²)**

ä¼½ç›åˆ†å¸ƒæ˜¯ç»Ÿè®¡å­¦çš„ä¸€ç§è¿ç»­æ¦‚ç‡å‡½æ•°ã€‚ä¼½ç›åˆ†å¸ƒä¸­çš„å‚æ•°Î±ï¼Œç§°ä¸ºå½¢çŠ¶å‚æ•°ï¼ŒÎ²ç§°ä¸ºå°ºåº¦å‚æ•°ï¼Œå‡è®¾éšæœºå˜æ•°Xä¸º
ç­‰åˆ°ç¬¬Î±ä»¶äº‹å‘ç”Ÿæ‰€éœ€ä¹‹ç­‰å€™æ—¶é—´ã€‚

æ”¯æ’‘é›† $x \in \lbrack 0,  + \infty)$

æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œå–$\lambda = 1/\beta$

$$f( x ) = \frac{x^{\alpha - 1}\lambda^{\alpha}e^{- \lambda x}}{\Gamma( \alpha )}\quad x > 0$$

Gammaå‡½æ•°ç‰¹å¾

$$\left\{ \begin{matrix}
Î“( \alpha ) = ( \alpha - 1 )! & \textrm{$ if\ \alpha\ is \mathbb{Z}^{+} $} \\
Î“( \alpha ) = ( \alpha - 1 )!\Gamma( \alpha - 1 ) & \textrm{$ if\ Î±\ is\ \mathbb{R}^{+} $} \\
\Gamma( 1/2  ) = \sqrt{\pi} & \textrm{$ \Gamma( 1 ) = 1 $} \\
\end{matrix} \right. $$

ç´¯è®¡åˆ†å¸ƒå‡½æ•°æ¯”è¾ƒå¤æ‚

æ•°å­¦æœŸæœ›

$$E( X ) = Î±Î²$$

æ–¹å·®

$$D( X ) = \beta^{2}\alpha$$

**è´å¡”åˆ†å¸ƒ X \~ Be(Î±, Î²)**

æŒ‡ä¸€ç»„å®šä¹‰åœ¨(0,1)åŒºé—´çš„è¿ç»­æ¦‚ç‡åˆ†å¸ƒï¼Œæœ‰ä¸¤ä¸ªå‚æ•°Î±ï¼ŒÎ²\>0ã€‚

æ”¯æ’‘é›† $x \in (0, 1)$

æ¦‚ç‡å¯†åº¦å‡½æ•°

$$f( x ) = \frac{x^{\alpha - 1}{(1 - x)}^{\beta - 1}}{Î’( \alpha,\beta )}\quad x > 0$$

$$Î’( \alpha,\beta ) = \int_{0}^{1}{u^{\alpha - 1}{(1 - u)}^{\beta - 1}}du$$

Betaå‡½æ•°ç‰¹å¾

$$Î’( \alpha,\beta ) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}$$

ç´¯è®¡åˆ†å¸ƒå‡½æ•°æ¯”è¾ƒå¤æ‚

æ•°å­¦æœŸæœ›

$$E( X ) = \frac{\alpha}{\alpha + \beta}$$

æ–¹å·®

$$D( X ) = \frac{Î±Î²}{( \alpha + \beta )^{2}(\alpha + \beta + 1)} $$

**å…¶ä»–åˆ†å¸ƒ**

**æˆªæ–­æ­£æ€åˆ†å¸ƒ X \~ TN(n, p)**

**Dirichletåˆ†å¸ƒ X**

# æŠ½æ ·æ–¹æ³•

## The Inversion Method

### ç¦»æ•£å‹

å‡è®¾å¯¹äºä¸€ä¸ªéšæœºå˜é‡ X æœ‰æ¦‚ç‡è´¨é‡å‡½æ•° pmf

$$p(X=x_i)=p_i\quad \textrm{where $p_i>0, i=1, 2,\ldots, m$ and $\sum_{i=1}^{m}{p_i=1}$}$$

å…¶ä¸­ m å¯ä»¥æ˜¯ finite or infinite. 

**é‚£ä¹ˆ**ä» U(0, 1)ä¸­å‡åŒ€æŠ½æ ·ï¼Œå¾—åˆ° Y ä¸ X çš„æ¦‚ç‡ç´¯ç§¯åˆ†å¸ƒæ¯”è¾ƒï¼Œè½åœ¨å“ªä¸ªåŒºé—´å³å¯¹åº”å“ªä¸ªå€¼ã€‚Xçš„æ¦‚ç‡å–å€¼æœä»å‡åŒ€åˆ†å¸ƒã€‚

<div class="note primary no-icon">**ç®—æ³•è¿‡ç¨‹**

**Step1.** Draw Y from U(0,1);

**Step2.** If Y < p1, set X = x1 and stop;

Else if Y < p1+p2, set X = x2 and stop;

...

Else if Y < âˆ‘pi, set X = xi and stop;

...
</div>

<div class="note info">**Note.**

1. é€šå¸¸å°†è¾ƒå¤§ pi çš„æŠ½æ ·æ”¾åœ¨é è¿‘ 0 çš„ä¸€ä¾§ï¼Œä¼šæ›´é«˜æ•ˆã€‚
2. ä¸ä¸€å®šè¦å°†æ‰€æœ‰ pi éƒ½å…ˆè®¡ç®—å‡ºæ¥ã€‚æ¯”å¦‚ç”¨ Inversion Method ç”Ÿæˆæ³Šæ¾åˆ†å¸ƒæ ·æœ¬æ—¶ï¼Œå…ˆæ‰§è¡Œ Step1ï¼Œç„¶åä¾æ¬¡è®¡ç®—å‡º piï¼Œå†æ±‚å’Œæ¯”è¾ƒã€‚å¦‚æœç¬¦åˆæ¡ä»¶ï¼Œå°±ä¸å¿…è®¡ç®—åç»­çš„ pi äº†ã€‚</div>

### è¿ç»­å‹

å‡è®¾å¯¹äºä¸€ä¸ªéšæœºå˜é‡ X æœ‰æ¦‚ç‡ç´¯ç§¯åˆ†å¸ƒå‡½æ•° cdf

$$F(x)=Pr(X<x)=\int_{-\infty}^x f(t)dt $$

**é‚£ä¹ˆ**ä»¤ Y = y ~ U(0, 1)ã€‚ä» y = F(x)æ±‚è§£ x = F-1 (y)ã€‚F(x)æ˜¯æœä»(0, 1)ä¸Šçš„å‡åŒ€åˆ†å¸ƒçš„ã€‚

<div class="note primary no-icon">**ç®—æ³•è¿‡ç¨‹**

**Step1.** Get the cdf F(x) of X;

**Step2.** Let x = F-1(y);

**Step3.** Draw Y from U(0, 1);

**Step4.** Return X = F-1(Y).
</div>

<div class="note info">**Note.**

è¯æ˜éšæœºå˜é‡ X æœ‰è¿ç»­çš„ cdf, åˆ™éšæœºå˜é‡ Y=F(X)~U(0, 1) (è¿™é‡Œçš„ F(Â·)è¢«è§†ä½œä¸€ä¸ªå‡½æ•°)ã€‚åªéœ€è¯ F(y)=y

$$F(y)=Pr(Y \leq y)=Pr(F(X)\leq y)$$

å¯¹äº Yâ†’X çš„æ˜ å°„ï¼Œæœ‰

$$x=F^{-1}(y)=\text{inf}\{x:F(x)\geq y\}\quad \textrm{where $y\in (0,1)$}$$

æ˜“çŸ¥$F(F^{-1}(y))\equiv F(x)\equiv y$

$$Pr(F(X)\leq y)=Pr(X\leq x)=F(x)=F(F^{-1}(y))=y$$

å¾—è¯ã€‚</div>

## The Grid Method

### è¿ç»­å‹ Only

å‡è®¾å¯¹äºä¸€ä¸ªéšæœºå˜é‡ X æœ‰æ¦‚ç‡å¯†åº¦å‡½æ•° pdf

$$X \sim f_X(x)\quad \text{where $S_X$ is finite}$$

**é‚£ä¹ˆ**åœ¨æ”¯æ’‘é›† S_X ä¸Šé€‰å–åˆé€‚çš„æ ¼ç‚¹é›†åˆ {xi} è¦†ç›– S_X ã€‚è®¡ç®—è¯¥æ ¼ç‚¹é›†åˆå¯¹åº”çš„æ¦‚ç‡å¯†åº¦å‡½æ•°å€¼é›†åˆ {fx(xi)} å¹¶ä½œå½’ä¸€åŒ–

$$p_i=\frac{f_X(x_i)}{\sum_{i=1}^d{f_X(x_i)}}\quad i=1, 2, \ldots,d$$

å°±å¾—åˆ°äº†éšæœºå˜é‡ X çš„è¿‘ä¼¼ X'

$$X' \sim \text{FDiscrete}_d(\{x_i\},\{p_i\})$$

<div class="note primary no-icon">**ç®—æ³•è¿‡ç¨‹**

**Step1.** Generate an appropriate set {xi}, which covers $S_X$ ;

**Step2.** Get the {pi};

**Step3.** Sample from X' ~ FDiscrete({xi},{pi}).
</div>

<div class="note info">**Note.**

ä¸»è¦æ˜¯å¤„ç† IM è¿ç»­å‹è®¡ç®—å¤æ‚çš„æƒ…å†µï¼Œå°†å…¶ç®€åŒ–è‡³ç¦»æ•£å‹ï¼Œç„¶åå†ç”¨ IM æˆ–å…¶ä»–æ–¹æ³•æŠ½
æ ·ã€‚</div>

## The Rejection Method (The AR Method(Acceptance-Rejection))

### è¿ç»­å‹/ç¦»æ•£å‹

å‡è®¾å¯¹äºä¸€ä¸ªéšæœºå˜é‡ X æœ‰æ¦‚ç‡å¯†åº¦å‡½æ•° pdf

$$X \sim f_X(x)\quad \text{where $S_X$ is finite}$$

ä¸”æœ‰å¸¸æ•° câ‰¥1 å’ŒåŒ…ç»œå¯†åº¦å‡½æ•° g(x)ï¼Œæ»¡è¶³

$$f(x)\leq cg(x)\quad \forall x\in S_X$$

**é‚£ä¹ˆ**ç”±å¿…é¡»æ»¡è¶³çš„æ¡ä»¶ï¼š

$$f(x)\leq cg(x)\quad \forall x\in S_X$$

æ®æ­¤å¯ä»¥å¾—åˆ°

$$f_Y(x|Z\leq \frac{f(Y)}{cg(Y)})=\frac{Pr(Z\leq \frac{f(Y)}{cg(Y)}|Y=x)\cdot g(x)}{Pr(Z\leq \frac{f(Y)}{cg(Y)})}\\
=\frac{Pr(Z\leq \frac{f(Y)}{cg(Y)})}{\int_{S_X}{Pr(Z\leq \frac{f(Y)}{cg(Y)}|Y=x)}g(x)dx} g(x)\\
=\frac{\frac{f(x)}{cg(x)} g(x)}{\int_{S_X}{\frac{f(x)}{cg(x)}g(x)dx}}=\frac{\frac{f(x)}{c}}{1/c}=f(x)$$

ç”±ä¸Šå¼å¯çŸ¥ï¼Œä»…å½“

$$f(x)\leq cg(x)\quad \forall x\in S_X$$

æ»¡è¶³æ—¶ï¼ŒfY(x|Zâ‰¤f(Y)/cg(Y))=f(x)æ‰æˆç«‹ï¼Œå¦åˆ™ fY(x|Zâ‰¤f(Y)/cg(Y))=g(x)ã€‚æ‰€ä»¥å¾ˆè‡ªç„¶åœ°,

$$c=\{ c_{opt}|c_{opt}\geq \frac{f(x)}{g(x)}\quad \forall x\in S_X \}$$

ç”±äº

$$Pr(Z\leq \frac{f(Y)}{cg(Y)})=\int_{S_X} Pr(Z\leq \frac{f(Y)}{cg(Y)}\big|Y=x)g(x)dx\\
=\int_{S_X} \frac{f(x)}{cg(x)}g(x)dx=1/c $$

ä»£è¡¨äº†æŠ½æ ·æœ‰æ•ˆçš„æ•ˆç‡ï¼Œæ‰€ä»¥ c åº”å½“å°½å¯èƒ½å°ã€‚æ•…å¯¹äºä¸€ä¸ªæ»¡è¶³è¦æ±‚çš„å‡½æ•°ç°‡$g_{\theta}(x)$ï¼Œå–

$$c=min\{ max_\theta (\frac{f(x)}{g_{\theta}(x)}) \} $$

<div class="note primary no-icon">**ç®—æ³•è¿‡ç¨‹**

**Step1.** Draw Z ~ U(0, 1) and independently draw Y ~ g(Â·) ;

**Step2.** If Z â‰¤ f(Y) / [cg(Y)], return X = Y, otherwise, go to Step 1.
</div>

<div class="note info">**Note.**

1. æ±‚åŒ…ç»œå‚æ•° c ç¬¬ä¸€æ­¥å–æœ€å¤§å€¼æ˜¯è¦æ»¡è¶³å……åˆ†æ¡ä»¶ câ‰¥f(x)/g(x)ã€‚ç¬¬äºŒæ­¥å–æœ€å°å€¼æ˜¯å¯¹äºä¸€ç°‡æ»¡è¶³è¦æ±‚çš„ g(x)ï¼Œè¦è®©æ¥å—æ¦‚ç‡æœ€å°ã€‚
2. æ¥å—æ¦‚ç‡ä¸º 1/cã€‚
3. æ±‚ f(x)/g(x)æœ€å¤§å€¼æ—¶å¯ä»¥æ”¹ç”¨æ±‚ logf(x)-logg(x)æœ€å¤§å€¼ã€‚
4. g(x)çš„æ¡ä»¶æœ‰ï¼š1ï¼‰å’Œ f(x)ä¸€æ ·çš„æ”¯æ’‘é›†ï¼›2ï¼‰æ¯” f(x)æ›´å¤§çš„å˜åŒ–[variance/dispersion]ï¼›3ï¼‰ä» g(x)ä¸­æŠ½æ ·æ›´ç®€å•ã€‚
5. log-concave å‡½æ•°é€‚åˆç”¨ piece-wise exponential envelopesã€‚ï¼ˆlog-concave å°±æ˜¯ log åæ˜¯ concave å‡½æ•°ï¼Œå¯ä»¥ç”¨åˆ†æ®µæŒ‡æ•°å‡½æ•°æ¥ä½œä¸ºåŒ…ç»œï¼‰ã€‚

</div>

## The Sampling/Importance Resampling Method (The SIR Method)

### è¿ç»­å‹ only

å‡è®¾å¯¹äºä¸€ä¸ªéšæœºå˜é‡ X æœ‰æ¦‚ç‡å¯†åº¦å‡½æ•° pdf

$$X \sim f_X(x)\quad \text{where $S_X$ is finite}$$

ä»¥åŠä¸€ä¸ªç›¸åŒæ”¯æ’‘é›†ä¸Šçš„æ¦‚ç‡å¯†åº¦å‡½æ•°pdf

$$ X \sim g(x) $$

**é‚£ä¹ˆ**å·²çŸ¥

$$f(x)=\frac{f(x)}{g(x)} g(x) \quad \forall x \in S_X $$

åˆ™ä¹ˆå¯ä»¥å°†ä» f(x)çš„æŠ½æ ·è¿‡ç¨‹çœ‹æˆä¸¤ä¸ªè¿‡ç¨‹ï¼šY~g(x)å’Œ X~f(Y)/g(Y)ã€‚è‹¥è¦ä»¤æŠ½æ ·çš„ X æ¦‚ç‡åˆ†å¸ƒå®Œå…¨ç­‰åŒäº f(x)ï¼Œé¦–å…ˆåº”å½“æŠ½å‡ºä¸€ä¸ªæ•°é‡æ— ç©·å¤§çš„{Y}ï¼Œå†ç”±f(y)/g(y)æŠ½å‡º Xã€‚è¿™ä¸ªé—®é¢˜åè€Œå˜å¤æ‚äº†ã€‚æ‰€ä»¥å¼•å…¥è¯¯å·®ï¼Œä½¿å¾—

$$ w(X^j)=\frac{f(X^j)}{X^j} $$

$$ \omega_j(X^j)=\frac{w(X^j)}{\sum{w(X^j)}} $$

ç›¸å½“äºä½¿ç”¨äº† <span style="color:firebrick">The Grid Method</span>ã€‚å†è¿›è¡Œé‡æŠ½æ ·æŠ½å‡º m ä¸ªæ ·æœ¬ã€‚
å…³äºè¿‘ä¼¼çš„ç¨‹åº¦

$$ Pr(X^*\leq x^*) = \sum_{j=1}^J{\omega_j \times I(X^j\leq x^*)}\\
=\frac{\sum_{j=1}^J {[w(X^j)\times I(X^j\leq x^*)]}}{\sum{w(X^j)}}\\
=\frac{\frac{1}{J}\sum_{j=1}^J {[w(X^j)\times I(X^j\leq x^*)]}}{\frac{1}{J} \sum{w(X^j)}}
$$

å…¶ä¸­$I(x)$æ˜¯æŒ‡ç¤ºå‡½æ•°ï¼Œä¸”æœ‰

$$ \text{mean}(w(X^j))=\frac{\int {w(x)\times I(x\leq x^*)\times g(x) dx}}{\int w(x)g(x)dx} $$

æ•…

$$ \lim_{J\to \infty} Pr(X^*\leq x^*)=\frac{\int w(x)\times I(x\leq x^*)\times g(x)dx}{\int w(x)g(x)dx}\\
=\int_{-\infty}^{x^*} f(x)dx $$

<div class="note primary no-icon">**ç®—æ³•è¿‡ç¨‹**


**Step1.** Generate X1, X2, â€¦ , XJ ~iid~ g(Â·) <span style="color:firebrick">[Sampling step]</span>;

**Step2.** Select a subset $\{X^i\}_{i=1}^m$ from $\{X^j\}_{j=1}^J$ via resampling without replacement from the discrete distribution on {Xj} with probabilities {Ï‰j} <span style="color:firebrick">[Importanceresampling step]</span>.

</div>

<div class="note info">**Note.**

1. J/m â†’ âˆæ—¶å³æ˜¯å®Œå…¨ç­‰åŒï¼Œä¸€èˆ¬å– J/mâ‰¥10 æˆ– J/m=20 æ¯”è¾ƒæœ‰æ•ˆã€‚
2. SIR å¾—åˆ°çš„æ˜¯åŸåˆ†å¸ƒçš„<span style="color:firebrick">è¿‘ä¼¼</span>ï¼Œé‡æŠ½æ ·è¿‡ç¨‹é‡‡ç”¨äº†ç±»ä¼¼ The Grid Method ç±»ä¼¼çš„æ€æƒ³ã€‚

</div>

## The Stochastic Representation Method (The SR Method)

### è¿ç»­å‹/ç¦»æ•£å‹ 

å‡è®¾å¯¹äºä¸€ä¸ªéšæœºå˜é‡ X æœ‰æ¦‚ç‡å¯†åº¦å‡½æ•° pdf

$$X \sim f_X(x)\quad \text{where $S_X$ is finite}$$

**é‚£ä¹ˆ**å¦‚æœéšæœºå˜é‡ X å’Œ Y æœ‰ç›¸åŒçš„åˆ†å¸ƒï¼Œåˆ™è®°ä½œğ‘‹ â‡” ğ‘Œ ï¼ˆâ‡”ä¸Šæœ‰ä¸ª"d"ï¼Œä¸‹æ–‡ç›¸åŒï¼‰ï¼Œå…¶ä¸­ï¼Œç›¸åŒåˆ†å¸ƒçš„æ„æ€æ˜¯

$$ F(x)=Pr(X\leq x) \Leftrightarrow Pr(Y\leq y)=F(y)\quad \text{let $x=y$}$$

æ¯”å¦‚æŸä¸ª X çš„ç´¯ç§¯åˆ†å¸ƒ F(x)å…³äº(0, 0.5)å¯¹ç§°ï¼Œé‚£ä¹ˆå– Y=-Xï¼Œæœ‰ 

$$ F(y) =Pr(Y\leq y)=Pr(-X\leq x)\\
=Pr(X\geq -x)=1-Pr(X\leq -x)\\
=1-F(-x)=F(x) $$

æ‰€ä»¥Y=-X â‡” Xã€‚è‹¥ğ‘‹ â‡” ğ‘Œä¸”æœ‰ğ‘‹ = â„(ğ‘Œ)ï¼Œåˆ™æœ‰$f_Y(y)dy=f_X(x)dx$

$$ f_Y(y)=f_X(x)\Big|\frac{dx}{dy}\Big|=f_X(h(y))\Big|\frac{dh(x)}{dy}\Big|=f_X(h(y))|\Delta h(y)| $$

åŒç†ï¼Œä¸€ä¸ªyå¯¹åº”å¤šä¸ªxæ—¶ï¼Œ$X=h_i(Y)$

$$ f_Y(y)=\sum f_X(h_i(y))|\Delta h_i(y)| $$

<div class="note primary no-icon">**ç®—æ³•è¿‡ç¨‹**

è¿™é‡Œæ˜¯ä¸€å¯¹äºŒçš„æƒ…å†µã€‚

**Step1.** Draw Z âˆ¼ U(0, 1) and independently draw Y âˆ¼ fY (Â·);

**Step2.** Set X1 = h1(Y) and X2 = h2(Y);

**Step3.** If $Z\leq \{1+\frac{f_X(X_2)}{f_X(X_1)}\Big|\frac{\Delta G(X_1)}{\Delta G(X_2)}\Big|\}$, return X = X1, else return X = X2.

</div>

<div class="note info">**Note.**

1. é€†æ–¹æ³•å¯ä»¥çœ‹ä½œ SR æ–¹æ³•çš„ç‰¹ä¾‹ï¼šæ²¡æœ‰ Step3ï¼›
2. beta åˆ†å¸ƒå¯ä»¥é€šè¿‡ gamma åˆ†å¸ƒç”Ÿæˆ;
3. inverse gamma é€šè¿‡ gamma;
4. chi-squared and log-normal via normal;
5. Studentâ€™s t- and F-distribution via normal and chi-squaredï¼›
6. Dirichlet distribution å¯ä»¥é€šè¿‡ independent gamma distributions;
7. multivariate normal distribution é€šè¿‡ uni-normalï¼›
8. multivariate t-distribution é€šè¿‡ multinormal and chi-squared;
9. Wishart é€šè¿‡ multinormalã€‚

</div>

## The Conditional Sampling Method (The CS Method)

### è¿ç»­å‹/ç¦»æ•£å‹ 

å‡è®¾å¯¹äºä¸€ä¸ªéšæœºå˜é‡ $\vec X=(X_1,\ldots,X_d)^T$ æœ‰æ¦‚ç‡å¯†åº¦å‡½æ•° pdf

$$\vec X \sim f_X(\vec x)=f_X(x_1,x_2,\ldots,x_d)\quad \text{where $S_X$ is finite}$$

è¯¥å¼å¯ä»¥å†™ä½œ$f_X(x_1,x_2,\ldots,x_d)=f_1(x_1)\prod_{i=2}^d f_i(x_i|x_1,x_2,\ldots,x_{i-1})$ å¹¶æ±‚è§£ã€‚

<div class="note primary no-icon">**ç®—æ³•è¿‡ç¨‹**

**Step1.** Draw X1 from f1(x1);

**Step2.** Draw X2 from f2(x2|x1);

**Step3.** Draw X3 from f3(x3|x1, x2);

......

**Stepi.** Draw Xd from fd(xd|x1, x2, â€¦, xd-1).

</div>

<div class="note info">**Note.**

æš‚æ— ã€‚

</div>

# ç»Ÿè®¡ä¸­çš„ä¼˜åŒ–ç®—æ³•

## åŸºç¡€

## The Newton-Raphson Algorithm

## The Fisher Scoring Algorithm

## The EM Algorithm

## The ECM Algorithm

## The MM Algorithms

# ç»å…¸è’™ç‰¹å¡æ´›ç§¯åˆ†ä¸é»æ›¼å’Œä¼°è®¡

## Classical Monte Carlo Integration

## The Riemannian Sum Estimator

# è´å¶æ–¯ç»Ÿè®¡ Bayesian Statistics

å¾…æ›´

# éšæœºè¿‡ç¨‹

## é©¬å°”å¯å¤«é“¾

## å¹³ç¨³è¿‡ç¨‹

# è’™ç‰¹å¡æ´›æ–¹æ³• MCMC Methods

å¾…æ›´

# Bootstrap Methods

<div class="note warning">**What's the bootstrap?**</div>

Bootstrap æ˜¯ç»Ÿè®¡æ¨æ–­ä¸­åŸºäºæ•°æ®è®¡ç®—çš„æ–¹æ³•ã€‚

> The bootstrap is a data-based method for statistical inference. Its introduction into statistics is relatively recent because the method is computationally intensive.

<div class="note warning">**The purpose of the bootstrap.**</div>

1. Bootstrapæä¾›äº†ä¸€ç§é€šç”¨æ–¹æ³•æ¥è·å¾—ä¼°è®¡é‡çš„æ ‡å‡†è¯¯å·®ä¼°è®¡ï¼ˆ${\widehat {Se}(\hat \theta)}$ï¼‰ä»¥åŠå‚æ•°çš„ç½®ä¿¡åŒºé—´CIã€‚
2. BootstrapåŒæ ·åº”ç”¨äºå‡è®¾æ£€éªŒä¸­ã€‚

> 1. First, the bootstrap approach provides a general method for obtaining **estimated standard errors** ${\widehat {Se}(\hat \theta)}$ of estimators and **confidence intervals** $CI$ of parameters.
> 2. Second, the bootstrap approach can also be applied to **testing hypotheses** to calculate a bootstrap p-value or to provide an upper quantile point of the distribution of a test-statistic when its density is **not available in closed-form**.

## Parametric Bootstrap

<div class="note warning">**How to describe the parametric bootstrap for computing CIs of parameters of interest by one sentence?**</div>

- Suppose that we have a method to calculate the point estimator $\hat \theta$, repeatedly computing the point estimator G times based on G bootstrap samples will result in the CI of Î¸.
- Thus, the key is how to generate a bootstrap sample.

### An example: Large-sample CIs for one-sample problem

Let ${\{Xi\}}_{i=1}^{n}$  i.i.d. Bernoulli(Î¸), where $\theta = Pr(X_1 =1)$ is unknown parameter of population mean. 

Let $\vec{X}=(X_1,\cdots,X_n)^T$ and $\vec{x}=(x_1,\cdots,x_n)^T$. The likelihood function for Î¸ is:

$$L(\theta)=\prod_{i=1}^n{\theta^{x_i}(1-\theta)^{1-x_i}},\qquad 0\leq\theta\leq1$$

so that the MLE of Î¸ is the sample mean defined by:

$$\hat\theta=\frac{1}{n}\sum_{i=1}^n{x_i=s(\vec{x})}$$

Note that

$$n\hat\theta=\sum_{i=1}^n{X_i}\sim{Binomial(n,\theta)}$$ 

then

$$E(\hat\theta)=\theta,\qquad and \qquad Var(\hat\theta)=\theta(1-\theta)/n$$

According to **Central Limit Theorem**, we have

$$\frac{\hat\theta-\theta}{\sqrt{\theta(1-\theta)/n}}\to Z \sim N(0,1)$$

Namely, $[\hat\theta-E(\hat\theta)]/[Var(\hat\theta)]^\frac{1}{2}$ **coverges in distribution** to a random variable following N(0,1). Based on limiting properties of MLE, we have approximately

$$\frac{\hat\theta-\theta}{\sqrt{\theta(1-\theta)/n}} \thicksim N(0,1)\qquad as \qquad n \to \infty$$

Let $z_\alpha$ denote the upper $\alpha$-th quantile of N(0,1) satisfying $Pr(Z \geq z_\alpha)=\alpha$. Therefore, an asymptotic $100(1-\alpha)\%$ confidence interval (CI) of Î¸ is given by:

$$1-\alpha=Pr(-z_{\alpha/2} \leq \frac{\hat\theta-\theta}{\hat\sigma} \leq z_{\alpha/2})=Pr(\hat\theta-z_{\alpha/2}\hat\sigma \leq \theta \leq \hat\theta + z_{\alpha/2}\hat\sigma)$$

Thus, $[\hat\theta_l, \hat\theta_u]=[\hat\theta-z_{\alpha/2}\hat\sigma,\hat\theta+z_{\alpha/2}\hat\sigma]$, where $\hat\sigma=\sqrt{\hat\theta(1-\hat\theta)/n}$

<div class="note warning">
**Two problems with the asymptotic CI**

1. å³ä¾¿å¯¹äºå¤§å®¹é‡æ ·æœ¬ï¼Œå½“Î¸çœŸå®å€¼æ¥è¿‘0æ—¶ï¼ŒCIä¸‹ç•Œä¼šä½äº0ï¼›å½“Î¸çœŸå®å€¼æ¥è¿‘1æ—¶ï¼ŒCIä¸‹ç•Œä¼šé«˜äº1ã€‚è¿™æ˜¯æ— æ•ˆçš„ã€‚
2. å¯¹äºå°åˆ°ä¸­ç­‰å®¹é‡æ ·æœ¬ï¼Œasymptotic CIä¸å¤Ÿå¯é ã€‚

> 1. First, even though for large sample size n, the lower bound may be beyond zero when the true value of Î¸ is close to zero while the upper bound may be beyond 1 when the true value of Î¸ is near to 1.
> 2. Second, for small to moderate sample sizes, the asymptotic CI is not reliable.

</div>

### Parametric Bootstrap World

å‡è®¾æœ‰åˆ†å¸ƒ $x \sim f(x;\theta)$

<div class="note primary no-icon">**ç®—æ³•è¿‡ç¨‹**

**Step1.** è®¡ç®—å‚æ•°çš„ç‚¹ä¼°è®¡(point estimator) $\hat\theta=s(\vec x)$ï¼Œæ¯”å¦‚é‡‡ç”¨MLEã€‚

**Step2.** ç”Ÿæˆç‹¬ç«‹åŒåˆ†å¸ƒbootstrap sample $\vec X^*=(x_1^*,\cdots,x_n^*)^T$ with $\{x_i^*\}_{i=1}^n\sim f(x;\hat\theta)$ å¹¶ä¸”è®¡ç®—å¯¹åº”çš„bootstrap replication $\hat\theta^*=s(\vec x^*)$.

**Step3.** ç‹¬ç«‹åœ°é‡å¤Gæ¬¡ Step2 ï¼Œè·å¾—G bootstrap replication $\{\hat\theta^*(g)\}_{g=1}^G$.

**Step4.** ä¸æ­¤åŒæ—¶ï¼Œæ ‡å‡†è¯¯å·® ${Se}(\hat \theta)$ å¯ä»¥é€šè¿‡the G replicationæ¥ä¼°è®¡ï¼Œå¦‚

$$\widehat {Se}^*(\hat \theta)=\sqrt{\frac{1}{G-1}\sum_{g=1}^G[\hat\theta^*(g)-\overline \theta^*]^2} \\  where \qquad \overline\theta^*=[\hat\theta^*(1)+\cdots+\hat\theta^*(G)]/G$$

**Step5.** å¦‚æœ$\{\hat\theta^*(g)\}_{g=1}^G$è¿‘ä¼¼æ­£æ€åˆ†å¸ƒï¼Œåˆ™$100(1-\alpha)\%$ bootstrap CI of $\theta$ ä¸º
$[\hat\theta_l^*, \hat\theta_u^*]=[\overline\theta^*-z_{\alpha/2}\widehat {Se}^*(\hat \theta),\overline\theta^*+z_{\alpha/2}\widehat {Se}^*(\hat \theta)]$

> **Note**: in standard normal distribution, $z_{\alpha/2} \approx 1.96$.

**Step6.** å¦‚æœè¿œéæ­£æ€åˆ†å¸ƒæˆ–Step5 ä¸­çš„CIæ˜¯æ— æ•ˆçš„ï¼Œåˆ™$100(1-\alpha)\%$ bootstrap CI of $\theta$ ä¸º $[\hat\theta_L^*, \hat\theta_U^*]$ï¼Œå…¶ä¸­$\hat\theta_L^*$æ˜¯é¡ºåºï¼ˆé€’å¢ï¼‰ç»Ÿè®¡é‡$\{\hat\theta^*(g)\}_{g=1}^G$çš„ç¬¬$(\alpha/2)G$ ä¸ªä¼°è®¡å€¼ï¼Œè€Œ$\hat\theta_U^*$æ˜¯ç¬¬$(1-\alpha/2)G$ ä¸ªä¼°è®¡å€¼ã€‚

> **For example**, when $\alpha=0.05$ and G=1000, $\hat\theta_L^*$ is the 25-th order statistic and $\hat\theta_U^*$ is the 975-th order statistic of $\hat\theta^*(1),\cdots,\hat\theta^*(1000)$, respectively.

</div>

## Non-Parametric Bootstrap

<div class="note warning">**Why need the non-parametric bootstrap?**</div>

- In many real applications, the form of density function is **unknown**.
- It is desirable to use a non-parametric bootstrap method to obtain the estimated standard error of an estimator (e.g., **the least square estimator (LSE)**) or the BCI for a population parameter (e.g., the mean of population distribution).

The key is how to generate a bootstrap sample from **the empirical cdf**.

### Non-Parametric Bootstrap World

å‡è®¾åˆ†å¸ƒ(the empirical cdf)å½¢å¼ä¸º $x \sim \hat F_n(x;\theta)=\hat F_n(x)=\frac{1}{n} \sum_{i=1}^n I_{(x_i \leq x)}$  where we assume $x_1\leq x_2\leq \cdots \leq x_n$. åŸºäºfunction $\hat F_n$, $\theta$ä¼°è®¡é‡å¯ç”±$\hat\theta=T(\hat F_n)=s(\vec x)$è®¡ç®—å¾—åˆ°ã€‚

<div class="note primary no-icon">**ç®—æ³•è¿‡ç¨‹**

**Step1.** è®¡ç®—å‚æ•°çš„ç‚¹ä¼°è®¡(point estimator) $\hat\theta=s(\vec x)$ã€‚

**Step2.** ç”Ÿæˆç‹¬ç«‹åŒåˆ†å¸ƒbootstrap sample $\vec X^*=(x_1^*,\cdots,x_n^*)^T$ with $\{x_i^*\}_{i=1}^n\sim \hat F_n(x)$ å¹¶ä¸”è®¡ç®—å¯¹åº”çš„bootstrap replication $\hat\theta^*=s(\vec x^*)$.

**Step3.** ç‹¬ç«‹åœ°é‡å¤Gæ¬¡ Step2 ï¼Œè·å¾—G bootstrap replication $\{\hat\theta^*(g)\}_{g=1}^G$.

**Step4.** ä¸æ­¤åŒæ—¶ï¼Œæ ‡å‡†è¯¯å·®${Se}(\hat \theta)$å¯ä»¥é€šè¿‡the G replicationæ¥ä¼°è®¡ï¼Œå¦‚

$$\widehat {Se}^*(\hat \theta)=\sqrt{\frac{1}{G-1}\sum_{g=1}^G[\hat\theta^*(g)-\overline \theta^*]^2} \\  where \qquad \overline\theta^*=[\hat\theta^*(1)+\cdots+\hat\theta^*(G)]/G$$

**Step5.** å¦‚æœ$\{\hat\theta^*(g)\}_{g=1}^G$è¿‘ä¼¼æ­£æ€åˆ†å¸ƒï¼Œåˆ™$100(1-\alpha)\%$ bootstrap CI of $\theta$ ä¸º
$[\hat\theta_l^*, \hat\theta_u^*]=[\overline\theta^*-z_{\alpha/2}\widehat {Se}^*(\hat \theta),\overline\theta^*+z_{\alpha/2}\widehat {Se}^*(\hat \theta)]$

> **Note**: in standard normal distribution, $z_{\alpha/2} \approx 1.96$.

**Step6.** å¦‚æœè¿œéæ­£æ€åˆ†å¸ƒæˆ–Step5 ä¸­çš„CIæ˜¯æ— æ•ˆçš„ï¼Œåˆ™$100(1-\alpha)\%$ bootstrap CI of $\theta$ ä¸º $[\hat\theta_L^*, \hat\theta_U^*]$ï¼Œå…¶ä¸­$\hat\theta_L^*$æ˜¯é¡ºåºï¼ˆé€’å¢ï¼‰ç»Ÿè®¡é‡$\{\hat\theta^*(g)\}_{g=1}^G$çš„ç¬¬$(\alpha/2)G$ ä¸ªä¼°è®¡å€¼ï¼Œè€Œ$\hat\theta_U^*$æ˜¯ç¬¬$(1-\alpha/2)G$ ä¸ªä¼°è®¡å€¼ã€‚

</div>

<div class="note info">**æ³¨:** 

ä»$\hat F_n(x)$ ä¸­æŠ½å–ç‹¬ç«‹åŒåˆ†å¸ƒçš„bootstrap samples $\vec x^*=(x_1^*,\cdots,x_n^*)^T$ã€‚äº‹å®ä¸Šï¼Œç”±äºæŠ½å–$x_i^*$çš„è¿‡ç¨‹ç›¸å½“äºæœ‰æ”¾å›æŠ½æ ·ï¼Œå¿…æœ‰å…¶æŠ½æ ·æ ·æœ¬ç­‰äºåŸæ ·æœ¬ä¸­æŸä¸€æ ·æœ¬ï¼Œæ¯”å¦‚ $x_1^*=x_7, x_2^*=x_3, \cdots,x_n^*=x_7$ã€‚è¿™è¯´æ˜bootstrap sampleæ˜¯ç”±åŸæ ·æœ¬(the original observations)ç»„æˆçš„ã€‚æŸäº›æ ·æœ¬å¯èƒ½å‡ºç°0æ¬¡ã€1æ¬¡ã€2æ¬¡ç”šè‡³æ›´å¤šã€‚</div>

## Hypothesis Testing with the Bootstrap

<p style="font-family:é»‘ä½“;text-align:center">å¾…æ•´ç†...</p>

<!-- Last SVG -->
![](https://img.shields.io/badge/last--updated-2019.2.24-blue.svg)